{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5f535b-03f7-47ff-b5e5-9818b6b2ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1174a708-ad11-4ef9-88f2-a398afe75f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_softmax_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b873d2d-931b-4725-885c-a6eed0fe0c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1192, 0.8808],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.8808, 0.1192]], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_softmax_cuda.safe_softmax(torch.tensor([[1.0, 3.0], [2.0, 2.0], [3.0, 1.0]], device=\"cuda:0\", dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "099f8880-b64b-4395-8a7b-a95a96f21f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing softmax on tensor of shape torch.Size([128, 512]), strides (512, 1)\n",
      "Match!\n",
      "\n",
      "Testing softmax on tensor of shape torch.Size([128, 512]), strides (1, 128)\n",
      "Match!\n",
      "\n",
      "Original sliced tensor strides: (256, 2)\n",
      "\n",
      "Testing softmax on tensor of shape torch.Size([256, 128]), strides (256, 2)\n",
      "Match!\n",
      "\n",
      "Testing softmax on tensor of shape torch.Size([64, 64]), strides (64, 1)\n",
      "Match!\n"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "def test_softmax_cuda(x):\n",
    "    # Ensure x is on CUDA\n",
    "    if x.device.type != 'cuda':\n",
    "        x = x.cuda()\n",
    "    print(f\"\\nTesting softmax on tensor of shape {x.shape}, strides {x.stride()}\")\n",
    "    cuda_output = custom_softmax_cuda.safe_softmax(x)\n",
    "    torch_output = torch.softmax(x.float(), dim=-1) # Softmax over the last dimension\n",
    "\n",
    "    # Compare\n",
    "    if torch.allclose(cuda_output, torch_output, atol=1e-5):\n",
    "        print(\"Match!\")\n",
    "    else:\n",
    "        print(\"Mismatch!\")\n",
    "\n",
    "\n",
    "# 1. Row-major (default PyTorch)\n",
    "# Softmax over the last dimension (columns) is natural for row-major.\n",
    "x_row_major = torch.randn(128, 512, device='cuda', dtype=torch.float32)\n",
    "test_softmax_cuda(x_row_major)\n",
    "\n",
    "# 2. Column-major for the 'row' dimension (i.e., transposed from default)\n",
    "# If you want to perform softmax over the 'rows' of a logically column-major matrix,\n",
    "# you should transpose it first to make the 'rows' the last dimension in PyTorch.\n",
    "# Or, if your original matrix is (M, N) and you want softmax over dim 0 (rows),\n",
    "# you can transpose it to (N, M) and then apply softmax over dim 1.\n",
    "x_col_major_logical = torch.randn(512, 128, device='cuda', dtype=torch.float32)\n",
    "# To apply softmax over dim 0 (rows) of x_col_major_logical,\n",
    "# you would effectively transpose it to make the 'rows' the last dimension\n",
    "# for the Triton kernel which expects softmax over the last dim.\n",
    "# So, x_col_major_logical.T becomes (128, 512) and is row-major.\n",
    "test_softmax_cuda(x_col_major_logical.T)\n",
    "\n",
    "# Another example: a non-contiguous tensor due to slicing\n",
    "x_sliced = torch.randn(256, 256, device='cuda', dtype=torch.float32)[:, ::2] # x_sliced is (256, 128) but non-contiguous\n",
    "print(f\"\\nOriginal sliced tensor strides: {x_sliced.stride()}\")\n",
    "# The current kernel works fine with non-contiguous strides as it uses the provided strides.\n",
    "test_softmax_cuda(x_sliced)\n",
    "\n",
    "# A more complex permutation\n",
    "x_permuted = torch.randn(8, 64, 8, device='cuda', dtype=torch.float32).permute(0, 2, 1) # (8, 32, 256)\n",
    "# To apply softmax over the last dimension (64), this is fine.\n",
    "# If we wanted softmax over the middle dimension (128), we'd need to permute again\n",
    "# or re-design the kernel to iterate over a different stride.\n",
    "test_softmax_cuda(x_permuted.reshape(-1, x_permuted.shape[-1])) # Flatten to 2D for the current kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a78bcd-4e14-48c7-a9ed-bef8a85d5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.cpp_extension import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5edd276e-c4e7-41ef-9434-8708172651cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "jit_softmax = load(\"jit_softmax\", sources=[\"online_softmax.cu\", \"safe_softmax.cu\", \"binding.cpp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056a4bb0-efb1-4dc8-8bde-91a2a2d4aa21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(jit_softmax.safe_softmax(torch.randn(512, 128, device='cuda', dtype=torch.float32)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3cf67-a8c6-4c69-bc21-a63f0a5f123f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
