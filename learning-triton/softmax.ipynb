{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ebb99e-2eda-4c83-805e-adb117a6de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "from triton import language as tl\n",
    "import numpy as np\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc55ce9-8228-475b-a441-4e9a1a012730",
   "metadata": {},
   "source": [
    "First, let's implement softmax in numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "681ec9d9-429a-4442-9a87-c1009f7b4cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_np(x: npt.NDArray[np.float32]) -> npt.NDArray[np.float32]:\n",
    "    x_exp = np.exp(x)\n",
    "    return x_exp / np.sum(x_exp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f58c54-fb92-4d86-b606-cdbfb7927d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09003057, 0.66524096],\n",
       "       [0.24472847, 0.24472847],\n",
       "       [0.66524096, 0.09003057]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_np(np.array([[1.0, 3.0], [2.0, 2.0], [3.0, 1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf324fa-2ac3-416f-9256-c3fe5343185e",
   "metadata": {},
   "source": [
    "Then let's implement softmax in Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac01e56-756e-4571-8bff-7933b907d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax_kernel(x_ptr, output_ptr, x_num_rows: tl.constexpr, x_num_columns: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(0) # pid will be the column index, we parallelize across row_index and compute an entire column\n",
    "    offsets = tl.arange(0, BLOCK_SIZE) * x_num_columns + pid\n",
    "    mask = offsets < (x_num_rows * x_num_columns)\n",
    "    \n",
    "    x = tl.load(x_ptr + offsets, mask=mask, other=-float('inf'))\n",
    "    x_exp = tl.exp(x)\n",
    "    denom = tl.sum(x_exp, axis=0)\n",
    "    y = x_exp / denom\n",
    "\n",
    "    tl.store(output_ptr + offsets, y, mask=mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229d8b1f-c0f6-4367-90c1-64eeabc95c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_triton(x: torch.Tensor) -> torch.Tensor:\n",
    "    y = torch.empty_like(x)\n",
    "    x_num_cols = x.shape[1]\n",
    "    \n",
    "    grid = lambda meta: (x_num_cols, )\n",
    "    softmax_kernel[grid](x, y, x.shape[0], x.shape[1], BLOCK_SIZE=1024)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3946f65-1702-4367-ba56-0f4b0d9b7bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.6652],\n",
       "        [0.2447, 0.2447],\n",
       "        [0.6652, 0.0900]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_triton(torch.tensor([[1.0, 3.0], [2.0, 2.0], [3.0, 1.0]], device=\"cuda:0\", dtype=torch.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
