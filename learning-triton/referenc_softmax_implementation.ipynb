{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778a111f-49b1-469b-b345-9f45daf8d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel(\n",
    "    output_ptr, input_ptr,\n",
    "    M, N, # M rows, N columns\n",
    "    input_row_stride, input_col_stride, # Strides of the input tensor\n",
    "    output_row_stride, output_col_stride, # Strides of the output tensor\n",
    "    BLOCK_SIZE: tl.constexpr # N is the dimension we iterate over\n",
    "):\n",
    "    # We tile across the rows\n",
    "    row_idx = tl.program_id(0)\n",
    "\n",
    "    # Calculate pointer to the start of the current row\n",
    "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "\n",
    "    # Create a block of pointers for the current row\n",
    "    # This assumes we want to iterate over the columns (N) for softmax\n",
    "    # The 'columns' are accessed via input_col_stride\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    input_ptrs = row_start_ptr + col_offsets * input_col_stride\n",
    "\n",
    "    # Load a block of elements\n",
    "    # We use a mask to handle rows that are not a multiple of BLOCK_SIZE\n",
    "    mask = col_offsets < N\n",
    "    block = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "\n",
    "    # Compute softmax\n",
    "    numerator = tl.exp(block - tl.max(block, axis=0))\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    softmax_output = numerator / denominator\n",
    "\n",
    "    # Store the result\n",
    "    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "    output_ptrs = output_row_start_ptr + col_offsets * output_col_stride\n",
    "    tl.store(output_ptrs, softmax_output, mask=mask)\n",
    "\n",
    "class Softmax(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        M, N = x.shape\n",
    "        output = torch.empty_like(x)\n",
    "\n",
    "        # Get strides from PyTorch tensor\n",
    "        # PyTorch returns strides in elements, not bytes, so we don't need x.element_size() here\n",
    "        input_row_stride, input_col_stride = x.stride()\n",
    "        output_row_stride, output_col_stride = output.stride()\n",
    "\n",
    "        # BLOCK_SIZE must be a power of 2 for optimal performance\n",
    "        # We need to choose a BLOCK_SIZE that is at least N, or tile if N is larger\n",
    "        # For simplicity, let's assume N <= 1024 and pick a suitable block size.\n",
    "        # For larger N, you would need multiple blocks per row.\n",
    "        BLOCK_SIZE = triton.next_power_of_2(N)\n",
    "        if BLOCK_SIZE > 2048: # Cap block size to avoid register pressure\n",
    "            BLOCK_SIZE = 2048\n",
    "        # If N is small, a smaller block size might be better\n",
    "        if N < 64:\n",
    "             BLOCK_SIZE = 64\n",
    "\n",
    "        # Number of programs is equal to the number of rows (M)\n",
    "        grid = lambda meta: (M,)\n",
    "\n",
    "        softmax_kernel[grid](\n",
    "            output, x,\n",
    "            M, N,\n",
    "            input_row_stride, input_col_stride,\n",
    "            output_row_stride, output_col_stride,\n",
    "            BLOCK_SIZE=BLOCK_SIZE\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b23dc6-410f-47f4-b57c-452ad9b47628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing softmax on tensor of shape torch.Size([128, 512]), strides (512, 1)\n",
      "Match!\n",
      "\n",
      "Testing softmax on tensor of shape torch.Size([128, 512]), strides (1, 128)\n",
      "Match!\n",
      "\n",
      "Original sliced tensor strides: (256, 2)\n",
      "\n",
      "Testing softmax on tensor of shape torch.Size([256, 128]), strides (256, 2)\n",
      "Match!\n",
      "\n",
      "Testing softmax on tensor of shape torch.Size([4096, 64]), strides (64, 1)\n",
      "Match!\n"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "def test_softmax_triton(x):\n",
    "    # Ensure x is on CUDA\n",
    "    if x.device.type != 'cuda':\n",
    "        x = x.cuda()\n",
    "    print(f\"\\nTesting softmax on tensor of shape {x.shape}, strides {x.stride()}\")\n",
    "    triton_output = Softmax.apply(x)\n",
    "    torch_output = torch.softmax(x.float(), dim=-1) # Softmax over the last dimension\n",
    "\n",
    "    # Compare\n",
    "    assert torch.allclose(triton_output, torch_output, atol=1e-5), \\\n",
    "        f\"Mismatch!\\nTriton:\\n{triton_output}\\nTorch:\\n{torch_output}\"\n",
    "    print(\"Match!\")\n",
    "\n",
    "# 1. Row-major (default PyTorch)\n",
    "# Softmax over the last dimension (columns) is natural for row-major.\n",
    "x_row_major = torch.randn(128, 512, device='cuda', dtype=torch.float32)\n",
    "test_softmax_triton(x_row_major)\n",
    "\n",
    "# 2. Column-major for the 'row' dimension (i.e., transposed from default)\n",
    "# If you want to perform softmax over the 'rows' of a logically column-major matrix,\n",
    "# you should transpose it first to make the 'rows' the last dimension in PyTorch.\n",
    "# Or, if your original matrix is (M, N) and you want softmax over dim 0 (rows),\n",
    "# you can transpose it to (N, M) and then apply softmax over dim 1.\n",
    "x_col_major_logical = torch.randn(512, 128, device='cuda', dtype=torch.float32)\n",
    "# To apply softmax over dim 0 (rows) of x_col_major_logical,\n",
    "# you would effectively transpose it to make the 'rows' the last dimension\n",
    "# for the Triton kernel which expects softmax over the last dim.\n",
    "# So, x_col_major_logical.T becomes (128, 512) and is row-major.\n",
    "test_softmax_triton(x_col_major_logical.T)\n",
    "\n",
    "# Another example: a non-contiguous tensor due to slicing\n",
    "x_sliced = torch.randn(256, 256, device='cuda', dtype=torch.float32)[:, ::2] # x_sliced is (256, 128) but non-contiguous\n",
    "print(f\"\\nOriginal sliced tensor strides: {x_sliced.stride()}\")\n",
    "# The current kernel works fine with non-contiguous strides as it uses the provided strides.\n",
    "test_softmax_triton(x_sliced)\n",
    "\n",
    "# A more complex permutation\n",
    "x_permuted = torch.randn(32, 64, 128, device='cuda', dtype=torch.float32).permute(0, 2, 1) # (32, 128, 64)\n",
    "# To apply softmax over the last dimension (64), this is fine.\n",
    "# If we wanted softmax over the middle dimension (128), we'd need to permute again\n",
    "# or re-design the kernel to iterate over a different stride.\n",
    "test_softmax_triton(x_permuted.reshape(-1, x_permuted.shape[-1])) # Flatten to 2D for the current kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ebdb6-3a6e-4e65-a0b1-cf8e19b7a134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
