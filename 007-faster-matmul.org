* Faster matmul
There is a great write up by an engineer at Anthropic:
[[https://siboehm.com/articles/22/CUDA-MMM][blog post]] and [[https://github.com/siboehm/SGEMM_CUDA][Git repo]].

** Naive implementation
We launch a grid and threads along the matrix C. Basically
- ~i = blockIdx.y * blockDim.y + threadIdx.y~ (row index)
- ~j = blockIdx.x * blockDim.x + threadIdx.x~ (column index)

This results in the following kernel:
#+begin_src cuda
  __global__ void naive_matmul(float* A, float* B, float* C, int M, int N, int K) {
    int i = blockIdx.y * blockDim.y + threadIdx.y; // row index
    int j = blockIdx.x * blockDim.x + threadIdx.x; // column index

    if (i < M && j < N) {
      float sum = 0.0f;
      for (int k = 0; k < K; k++) {
        sum += A[i * K + k] * B[k * N + j];
      }
      C[i * N + j] = sum;
    }
  }
#+end_src

Given 2 4096 * 4096 matrices, let's calculate the number of:
- Compute: every elements will need to multiply & add with other matrix, so ~2 * 4096^3 = 137 GFLOPS~
- Theoretical read limit: ~2 * 4096^2 * 4 bytes = 128 MiB~
- Theoretical data to write: ~4096^2 * 4 bytes = 64 MiB~

On H100, it has 67 teraFLOPS / s, so in theory it can be done in ~1.9 ms~ for compute.
On H100, memory bandwidth is 3.35 TB/s, so ~40~ microseconds of memory latency. Compute is 475 times bigger

Compute / Memory ratio is 47.5. This is still compute bound in theory. If we can use < 47.5x memory access,
we would be above to achieve the best performance.

** Global memory Coalescing

32 threads in the consecutive ~threadId = threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.x * blockDim.y~ form a warp.
Within each warp, we want to have locality in terms of memory access across threads.

For example, thread 0 and thread 1 should access memory near each other, and ideally all the memory accessed by 32 threads
in the warp should become a consecutive block in global memory.

The concept of a warp is relevant for this second kernel,
as sequential memory accesses by threads that are part of the same warp can be grouped and executed as one.

In the naive kernel, each thread accesses different rows of B in the inner loop, leading to non-coalesced access.
The key insight is that the naive kernel already has the correct indexing for coalescing:

#+begin_src cuda
  __global__ void coalesced_matmul(float* A, float* B, float* C, int M, int N, int K) {
    int i = blockIdx.y * blockDim.y + threadIdx.y; // row index
    int j = blockIdx.x * blockDim.x + threadIdx.x; // column index

    if (i < M && j < N) {
      float sum = 0.0f;
      for (int k = 0; k < K; k++) {
        // A matrix access is broadcasted (same element to all threads in warp)
        // B matrix access is coalesced (consecutive threads access consecutive elements)
        sum += A[i * K + k] * B[k * N + j];
      }
      // Write is also coalesced
      C[i * N + j] = sum;
    }
  }
#+end_src

The optimization comes from swapping the grid dimensions in the kernel launch:
- Naive: ~dim3 grid(M/32, N/32)~
- Coalesced: ~dim3 grid(N/32, M/32)~ (swapped)



** Shared memory tiling
*** Memory access latency hierarchy:
- Register: ~1 cycle
- Shared memory: ~20-30 cycles
- L1/L2 cache: ~100-200 cycles
- Global memory: ~200-800 cycles

*** We can effectively cache blocks in shared memory
Using shared memory reduces global memory accesses from O(K) to O(K/BLOCK_SIZE) per thread.

#+begin_src cuda
  __global__ void sram_matmul(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int N, int K) {
    const int BLOCK_SIZE = 32;
    __shared__ float shared_A[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float shared_B[BLOCK_SIZE][BLOCK_SIZE];

    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    float sum = 0.0f;
    
    // Sliding window along K dimension
    for (int tile = 0; tile < (K + BLOCK_SIZE - 1) / BLOCK_SIZE; tile++) {
        // Load tiles into shared memory
        if (row < M && (tile * BLOCK_SIZE + threadIdx.x) < K) {
            shared_A[threadIdx.y][threadIdx.x] = A[row * K + tile * BLOCK_SIZE + threadIdx.x];
        } else {
            shared_A[threadIdx.y][threadIdx.x] = 0.0f;
        }

        if (col < N && (tile * BLOCK_SIZE + threadIdx.y) < K) {
            shared_B[threadIdx.y][threadIdx.x] = B[(tile * BLOCK_SIZE + threadIdx.y) * N + col];
        } else {
            shared_B[threadIdx.y][threadIdx.x] = 0.0f;
        }

        __syncthreads();

        // Compute dot product for this tile
        #pragma unroll
        for (int k = 0; k < BLOCK_SIZE; k++) {
            sum += shared_A[threadIdx.y][k] * shared_B[k][threadIdx.x];
        }

        __syncthreads();
    }

    if (row < M && col < N) {
        C[row * N + col] = sum;
    }
}
#+end_src
*** Profiling insights
When profiling with ~ncu --set full~, the kernel shows:
- High shared memory pressure due to many small accesses
- Long scoreboard stalls waiting for global memory loads
- Limited instruction-level parallelism (ILP)

This motivates the next optimization: giving each thread more work to increase ILP and reduce shared memory pressure.

** 1D block tiling kernel

Each thread computes TM elements (row-wise) instead of just one, increasing arithmetic intensity:

#+begin_src cuda
template <int BM, int BN, int BK, int TM>
__global__ void block_tiling_matmul_1d(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, 
                                       const int M, const int N, const int K) {
    __shared__ float shared_A[BM][BK];
    __shared__ float shared_B[BK][BN];

    // Thread mapping: linearize thread block and assign each thread TM rows
    const int thread_col = threadIdx.x % BN;
    const int thread_row = threadIdx.x / BN;

    // Move block tile to correct position
    A += blockIdx.y * BM * K;
    B += blockIdx.x * BN;
    C += blockIdx.y * BM * N + blockIdx.x * BN;

    // Calculate indices for loading tiles
    const uint inner_col_a = threadIdx.x % BK;
    const uint inner_row_a = threadIdx.x / BK;
    const uint inner_col_b = threadIdx.x % BN;
    const uint inner_row_b = threadIdx.x / BN;

    float thread_results[TM] = {0.0f};

    // Main loop over K dimension
    for (uint tile_k = 0; tile_k < K; tile_k += BK) {
        // Cooperative loading of tiles
        shared_A[inner_row_a][inner_col_a] = A[inner_row_a * K + inner_col_a];
        shared_B[inner_row_b][inner_col_b] = B[inner_row_b * N + inner_col_b];
        
        __syncthreads();
        
        A += BK;
        B += BK * N;

        // Compute dot products
        for (uint k = 0; k < BK; k++) {
            float b_val = shared_B[k][thread_col];  // Reused across TM iterations
            
            #pragma unroll
            for (uint m = 0; m < TM; m++) {
                thread_results[m] += shared_A[thread_row * TM + m][k] * b_val;
            }
        }
        __syncthreads();
    }

    // Write results
    #pragma unroll
    for (uint m = 0; m < TM; m++) {
        C[(thread_row * TM + m) * N + thread_col] = thread_results[m];
    }
}
#+end_src

Key improvements:
- Each thread computes TM=8 elements, amortizing shared memory reads
- B values are loaded once and reused TM times
- Better instruction-level parallelism through unrolled loops
- Typical configuration: BM=64, BN=64, BK=8, TM=8

** 2D block tiling

Further extends the 1D approach by having each thread compute a TM×TN tile (2D) instead of just TM elements (1D):

#+begin_src cuda
template <int BM, int BN, int BK, int TM, int TN>
__global__ void block_tiling_matmul_2d(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, 
                                       const int M, const int N, const int K) {
    __shared__ float shared_A[BM][BK];
    __shared__ float shared_B[BK][BN];

    // Each thread computes a TM×TN tile
    const uint thread_col = threadIdx.x % (BN / TN);
    const uint thread_row = threadIdx.x / (BN / TN);

    // Move to block's starting position
    A += blockIdx.y * BM * K;
    B += blockIdx.x * BN;
    C += blockIdx.y * BM * N + blockIdx.x * BN;

    // Indices for cooperative loading
    const uint inner_col_a = threadIdx.x % BK;
    const uint inner_row_a = threadIdx.x / BK;
    const uint inner_col_b = threadIdx.x % BN;
    const uint inner_row_b = threadIdx.x / BN;

    // Thread-local accumulators and registers
    float thread_results[TM][TN] = {0.0f};
    float reg_M[TM];
    float reg_N[TN];

    const uint stride_A = blockDim.x / BK;
    const uint stride_B = blockDim.x / BN;

    // Main K-dimension loop
    for (uint tile_k = 0; tile_k < K; tile_k += BK) {
        // Cooperative tile loading with strided access
        #pragma unroll
        for (int offset = 0; offset < BM; offset += stride_A) {
            shared_A[inner_row_a + offset][inner_col_a] = A[(inner_row_a + offset) * K + inner_col_a];
        }
        
        #pragma unroll
        for (int offset = 0; offset < BK; offset += stride_B) {
            shared_B[inner_row_b + offset][inner_col_b] = B[(inner_row_b + offset) * N + inner_col_b];
        }
        
        __syncthreads();
        
        A += BK;
        B += BK * N;

        // Compute using outer product formulation
        #pragma unroll
        for (uint k = 0; k < BK; k++) {
            // Load vectors into registers
            #pragma unroll
            for (uint m = 0; m < TM; m++) {
                reg_M[m] = shared_A[thread_row * TM + m][k];
            }

            #pragma unroll
            for (uint n = 0; n < TN; n++) {
                reg_N[n] = shared_B[k][thread_col * TN + n];
            }

            // Outer product
            #pragma unroll
            for (int m = 0; m < TM; m++) {
                #pragma unroll
                for (int n = 0; n < TN; n++) {
                    thread_results[m][n] += reg_M[m] * reg_N[n];
                }
            }
        }
        __syncthreads();
    }

    // Write results
    #pragma unroll
    for (int m = 0; m < TM; m++) {
        #pragma unroll
        for (int n = 0; n < TN; n++) {
            C[(thread_row * TM + m) * N + (thread_col * TN + n)] = thread_results[m][n];
        }
    }
}
#+end_src

Key improvements over 1D:
- Each thread computes TM×TN elements (typically 4×4)
- Uses outer product formulation for better register reuse
- Typical configuration: BM=64, BN=128, BK=64, TM=4, TN=4
- Better arithmetic intensity but higher register pressure

** Vectorized 2D block tiling

Improves memory bandwidth utilization by using vectorized loads/stores with float4 (128-bit):

#+begin_src cuda
// Vectorized loading example
const uint stride_A = blockDim.x * 4 / BK;  // 4 elements per float4
const uint stride_B = blockDim.x * 4 / BN;

// Load tiles with float4 (128-bit) operations
for (uint offset = 0; offset < BM; offset += stride_A) {
    reinterpret_cast<float4*>(&shared_A[inner_row_a + offset][inner_col_a])[0] = 
        reinterpret_cast<const float4*>(&A[(inner_row_a + offset) * K + inner_col_a])[0];
}

for (uint offset = 0; offset < BK; offset += stride_B) {
    reinterpret_cast<float4*>(&shared_B[inner_row_b + offset][inner_col_b])[0] = 
        reinterpret_cast<const float4*>(&B[(inner_row_b + offset) * N + inner_col_b])[0];
}

// Vectorized store (assuming TN is divisible by 4)
for (uint m = 0; m < TM; m++) {
    for (uint n = 0; n < TN; n += 4) {
        float4 result = make_float4(thread_results[m][n], thread_results[m][n+1], 
                                   thread_results[m][n+2], thread_results[m][n+3]);
        reinterpret_cast<float4*>(&C[(thread_row * TM + m) * N + (thread_col * TN + n)])[0] = result;
    }
}
#+end_src

Key improvements:
- 4× fewer memory transactions through vectorization
- Better memory bandwidth utilization
- Requires alignment and divisibility constraints
- Configuration: BM=128, BN=128, BK=8, TM=8, TN=8

** Warp tiling

Warp tiling adds an intermediate level of tiling between thread and block levels, explicitly organizing computation at the warp (32 threads) level.

*** Three levels of tiling hierarchy:

**** Block tiling
- Multiple blocks execute on different SMs
- Data reuse via shared memory (visible to all warps in block)
- Synchronization with ~__syncthreads()~
- Reduces global memory accesses from O(M×N×K) to O(M×N×K/BK)

**** Warp tiling  
- 32 threads in a warp work on a WM×WN sub-tile cooperatively
- Data reuse via register shuffle operations or shared memory
- No synchronization needed within warp (threads execute in lockstep)
- Reduces shared memory bank conflicts
- Better locality for register cache

**** Thread tiling
- Each thread computes TM×TN elements
- Data reuse via registers
- Increases instruction-level parallelism (ILP)
- Reduces indexing overhead

The warp-tiled kernels (e.g., ~sync_db_warp_matmul.cu~) organize computation so each warp processes its own tile within the larger block tile. Benefits include:
- Better shared memory access patterns
- Reduced bank conflicts through careful layout
- More efficient register usage
- Explicit control over warp-level parallelism

Typical configuration: WM=32, WN=64, with each warp computing multiple iterations (WMITER×WNITER).

** Double buffering – hiding memory latency

At this point our kernel is already compute-bound most of the time, but every
time we move to the next K-tile we still do the following strictly
sequentially:

1. Load A_tile and B_tile from global memory → shared memory
2. ~__syncthreads()~
3. Run the inner FMAs that use that tile
4. ~__syncthreads()~ so that the next load does not clobber the tile we are
   still reading from

The two synchronizations mean that all threads in the block are stalled
during the load phases – the SM issues no arithmetic instructions while the
memory pipelines are busy.

Double buffering (ping-pong buffering) removes the serialization by reserving 
two backing buffers in shared memory:

#+begin_example
|  A_0  |  B_0  |  A_1  |  B_1  |   ←   shared memory
            ↑          ↑
        compute   preload next tile
#+end_example

While warps perform FMAs using buffer 0, they simultaneously fill buffer 1 
with data for the next iteration. This creates a software pipeline:

#+begin_example
Iteration i :  [ load_i   ][ compute_i ]
Iteration i+1:            [ load_{i+1}   ][ compute_{i+1} ]
#+end_example

*** Without async copies (pre-Ampere)

The compiler and GPU scheduler overlap instruction streams:
- Thread issues ~ld.global~ followed by independent arithmetic instructions
- Hardware scoreboard allows both to be in flight simultaneously
- Provides instruction-level parallelism to hide memory latency
- Overlap is partial but still beneficial

*** With async copies (Ampere+)

Using ~cuda::memcpy_async~ provides:
- Instructions return immediately
- Dedicated LSU pipeline for copies
- Small in-flight queue per warp
- ~cuda::pipeline~ API for explicit producer/consumer relationships

Timeline with async copies:
#+begin_example
load_i (async)
                         compute_i (uses previous tile)
------------------------------------------------------------------------
consumer_wait()          load_{i+1} (async)              compute_{i+1}
#+end_example

Benefits: 1.2×–1.4× speedup on well-tiled kernels by completely overlapping
memory latency with computation.

** Tensor Memory Accelerator (TMA)

Hopper GPUs (H100) introduce hardware-accelerated 2D bulk copies between global 
and shared memory. TMA kernels use:

- ~CUtensorMap~ objects defining tiled views of matrices
- PTX-level barriers (~mbarrier~) for synchronization
- ~cp_async_bulk_tensor_2d~ instructions for hardware-managed transfers
- Prefetching to L2 cache
- Double buffering for latency hiding

Key advantages:
- Near-peak shared memory bandwidth
- Minimal software overhead
- Better than software-managed ~cp.async~
- Enables highest performance kernels (60%+ of theoretical peak)

** Benchmark results

Performance progression on H100 (4096×512 × 512×2048):

| Kernel | Performance | % of Peak | Speedup |
|--------|------------|-----------|---------|
| Naive | 0.49 TFLOPS | 0.7% | 1× |
| Coalesced | 6.28 TFLOPS | 9.4% | 12.7× |
| Shared Memory | 8.58 TFLOPS | 12.8% | 17.4× |
| 1D Tiling | 19.27 TFLOPS | 28.8% | 39.1× |
| 2D Tiling | 22.65 TFLOPS | 33.8% | 46.0× |
| Vectorized | 33.89 TFLOPS | 50.6% | 68.8× |
| Double Buffer | 36.54 TFLOPS | 54.5% | 74.2× |
| TMA | 38.24 TFLOPS | 57.1% | 77.6× |
| TMA + Warp Tile | 42.06 TFLOPS | 62.8% | 85.4× |

The progression shows how each optimization builds on the previous ones,
ultimately achieving over 90% of cuBLAS performance with the most optimized kernel.