* Faster matmul
There is a great write up by an engineer at Anthropic:
[[https://siboehm.com/articles/22/CUDA-MMM][blog post]] and [[https://github.com/siboehm/SGEMM_CUDA][Git repo]].

** Naive implementation
We launch a grid and threads along the matrix C. Basically
- ~x_C = blockIdx.x * blockDim.x + threadIdx.x~
- ~y_C = blockIdx.y * blockDim.y + threadIdx.y~

This results in the following kernel:
#+begin_src cuda
  __global__ void naive_matmul(float* A, float* B, float* C, int M, int N, int K) {
    int i = blockIdx.x * blockDim.x + threadIdx.x; // Moving fast in a warp
    int j = blockIdx.y * blockDim.y + threadIdx.y; // Not moving in a warp

    if (i < M && j < N) {
      float acc = 0.0f;
      for (int k = 0; k < K; k++) {
        acc += A[i * K + k] * B[k * N + j]
      }
      C[i * N + j] = acc;
    }
  }
#+end_src

Given 2 4096 * 4096 matrices, let's calculate the number of:
- Compute: every elements will need to multiply & add with other matrix, so ~2 * 4096^3 = 137 GFLOPS~
- Theoretical read limit: ~2 * 4096^2 * 4 bytes = 128 MiB~
- Theoretical data to write: ~4096^2 * 4 bytes = 64 MiB~

On H100, it has 67 teraFLOPS / s, so in theory it can be done in ~1.9 ms~ for compute.
On H100, memory bandwidth is 3.35 TB/s, so ~40~ microseconds of memory latency. Compute is 475 times bigger

Compute / Memory ratio is 47.5. This is still compute bound in theory. If we can use < 47.5x memory access,
we would be above to achieve the best performance.

** Global memory Coalescing

32 threads in the consecutive ~threadId = threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.x * blcokDIm.y~ can form a warp.
Within each warp, we want to have locality in terms of memory.
access across each threads.

For example, thread 0 and thread 1 should access memory near each other, and ideally all the memory accessed by 32 threads
in the warp should become a consecutive block in global memory.

The concept of a warp is relevant for this second kernel,
as sequential memory accesses by threads that are part of the same warp can be grouped and executed as one.

In the naive kernel, each threads, in the inner loop, is accessing the memory consecutively. However, across the wrap
the access is not consecutive.


All we need to do here is to swap the index of ~x~ and ~y~:
#+begin_src cuda
  __global__ void naive_matmul(float* A, float* B, float* C, int M, int N, int K) {
    int i = blockIdx.y * blockDim.y + threadIdx.y; // Let i remains the same in the warp
    int j = blockIdx.x * blockDim.x + threadIdx.x; // Let j moves in the same warp

    if (i < M && j < N) {
      float acc = 0.0f;
      for (int k = 0; k < K; k++) {
        // A matrix access is not coalesed but broadcasted
        // B matrix access is coalesced
        acc += A[i * K + k] * B[k * N + j]
      }
      // Write is also coalesced
      C[i * N + j] = acc;
    }
  }
#+end_src



** Shared memory tiling
*** Memory access latency:
- Register: 1 clock, 8 TB/s
- Shared memory: 32 clocks, 1.5 TB /s
- Local -> Global: 800 clocks, 200 GB / s
- Host: 5 GB/s
*** We can effectively cache the block in sram
#+begin_src cuda
  __global__ void sram_matmul(float* A, float* B, float* C, int M, int N, int K) {
    __shared__ float shared_A[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float shared_B[BLOCK_SIZE][BLOCK_SIZE];

    // threadIdx.x changes in the same warp
    int i = blockIdx.y * blockDim.y + threadIdx.y;
    int j = blockIdx.x * blockDim.x + threadIdx.x;


    if (i < M && j < N) {
        // sliding window of BLOCK_SIZE x BLOCK_SIZE
        // sliding on the K dimension
        float sum = 0.0f;
        for (int w_idx = 0; w_idx < (K + BLOCK_SIZE - 1) / BLOCK_SIZE; w_idx++) {
            int k_start = w_idx * BLOCK_SIZE;

            // init shared_A, shared_B and shared_C
            // Check bounds for shared_A and shared_B, add assign value to 0 if out of bounds
            if (k_start + threadIdx.x < K) {
                shared_A[threadIdx.y][threadIdx.x] = A[i * K + k_start + threadIdx.x];
            }

            if (k_start + threadIdx.y < K) {
                shared_B[threadIdx.y][threadIdx.x] = B[(k_start + threadIdx.y) * N + j];
            }

            __syncthreads(); // wait for all threads to finish loading data
            for (int k = 0; k < BLOCK_SIZE && (k_start + k) < K; k++) {
                sum += shared_A[threadIdx.y][k] * shared_B[k][threadIdx.x];
            }
            __syncthreads(); // avoid over-writing shared_A and shared_B before all threads finish the computation
        }
        C[i * N + j] = sum;
    }
}
#+end_src
*** Profiling the SRAM matmul kernel

Let's use the ~ncu --set full~ command to profile our kernel.
Also, use ~--generate-line-info~ for nvcc for line info.

Some of the suggestions from ncu:
**** MIO stalls
On average, each warp of this workload spends 10.2 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full.
This stall reason is high in cases of extreme utilization of the MIO pipelines,
which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure.
This stall type represents about 37.3% of the total average of 27.4 cycles between issuing two instructions.

The most problematic one is ~smsp__pcsamp_warps_issue_stalled_long_scoreboard~. This indicates that we spend a lot of time loading from global memory,
and we spend a lot of time loading from SRAM as well.

** 1D block tiling kernel

We can try to calculate 8 elements in a single thread to reduce the memory pressure for accessing B shared memory:
#+begin_src cuda
template <int BM, int BN, int BK, int TM>
__global__ void block_tiling_matmul_1d(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, const int M, const int N, const int K) {
    __shared__ float shared_A[BM][BK];
    __shared__ float shared_B[BK][BN];

    // blockIdx.x is the block id in the N dimension, aka the column index of the block
    // blockIdx.y is the block id in the M dimension, aka the row index of the block

    // Each warp will calculate 32 * TM elements, with 32 being the columnar dim.
    // Num threads = BM * BN / TM, we will 1d tiling on the M dimension.
    const int thread_col = threadIdx.x % BN;
    const int thread_row = threadIdx.x / BN;

    // Move blocktile to beginning of A's row and B's column
    A += blockIdx.y * BM * K;
    B += blockIdx.x * BN;
    C += blockIdx.y * BM * N + blockIdx.x * BN;

    const uint inner_col_a = threadIdx.x % BK; // warp-level GMEM coalescing
    const uint inner_row_a = threadIdx.x / BK;
    const uint inner_col_b = threadIdx.x % BN; // warp-level GMEM coalescing
    const uint inner_row_b = threadIdx.x / BN;

    float thread_results[TM] = {0.0f};

    // Assume K is divisible by BK
    for (uint bkIdx = 0; bkIdx < K; bkIdx += BK) {
        // Load A and B tiles into shared memory j
        shared_A[inner_row_a][inner_col_a] = A[inner_row_a * K + inner_col_a];
        shared_B[inner_row_b][inner_col_b] = B[inner_row_b * N + inner_col_b];

        __syncthreads();

        // advance blocktile
        A += BK;
        B += BK * N;

        // Perform matrix multiplication
        for (uint dot_idx = 0; dot_idx < BK; dot_idx++) {
            // This is cached & reused for each thread in the warp
            float tmp = shared_B[dot_idx][thread_col];

            // We are reading TM elemets from A[thread_row * TM : thread_row * TM + TM][dot_idx]
            // and multiply with the cached B[thread_col][dot_idx]
            #pragma unroll
            for (uint res_idx = 0; res_idx < TM; res_idx++) {
                thread_results[res_idx] += shared_A[thread_row * TM + res_idx][dot_idx] * tmp;
            }
        }
        __syncthreads();
    }

    #pragma unroll
    for (uint res_idx = 0; res_idx < TM; res_idx++) {
        C[(thread_row * TM + res_idx) * N + thread_col] = thread_results[res_idx];
    }
}
#+end_src


